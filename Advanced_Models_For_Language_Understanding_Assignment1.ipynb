{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"collapsed_sections":["7o3Mz8zr3foD"],"gpuType":"T4","include_colab_link":true},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11580317,"sourceType":"datasetVersion","datasetId":7260903},{"sourceId":11580322,"sourceType":"datasetVersion","datasetId":7260907}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"072a8241-d79e-4056-af84-28a0b1ec89d4","cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/AviRahimov/AMFLU_Assignment1/blob/main/Advanced_Models_For_Language_Understanding_Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github","colab_type":"text"}},{"id":"f27024dc-c7d3-4cfe-a614-16d1738a451b","cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T08:49:56.849922Z","iopub.execute_input":"2025-05-06T08:49:56.850167Z","iopub.status.idle":"2025-05-06T08:49:58.185447Z","shell.execute_reply.started":"2025-05-06T08:49:56.850143Z","shell.execute_reply":"2025-05-06T08:49:58.184693Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/test-no-target/test_no_target.csv\n/kaggle/input/train-df/train.csv\n","output_type":"stream"}],"execution_count":1},{"id":"f0a61948","cell_type":"markdown","source":"# ğŸ”¹ Assignment 1: Seq2Seq Model for Sentence Unshuffling\nIn this assignment, you'll implement a basic **Sequence-to-Sequence (Seq2Seq)** neural network using PyTorch.\n\n### ğŸ¯ **Your goal:**\nGiven a sentence whose words have been shuffled randomly, your model must reconstruct the original sentence.\n\n**Example:**\n\n| Input (shuffled) | Output (original) |\n|-----------------|------------------|\n| `mat the on sat cat The` | `The cat sat on the mat` |","metadata":{"id":"f0a61948"}},{"id":"37546a90","cell_type":"markdown","source":"## ğŸ”¹ Why this task?\nThis simple yet non-trivial task demonstrates how language models learn word-order and syntactic structures.\nFrom a psycholinguistic viewpoint, sentence reconstruction taps into:\n\n- **Working memory**: The model must hold multiple words and reorder them meaningfully.\n- **Syntax and semantics**: Reordering depends on syntactic constraints and semantic coherence.","metadata":{"id":"37546a90"}},{"id":"FmlLBejDCw6X","cell_type":"markdown","source":"## ğŸ”¹ Quick Summary of PyTorch Workflow\n\nThe general workflow when working with on deep learning with PyTorch usually involves these steps:\n\n1. **Prepare your data**:\n    - Define your dataset by subclassing `torch.utils.data.Dataset`.\n    - Use a `DataLoader` to iterate efficiently over the dataset in batches.\n    - Tokenization - select tokenization method and tokenize your data.\n\n2. **Define your model**:\n    - Create a model class by subclassing `nn.Module`.\n    - Define model layers in `__init__`.\n    - Define how data flows through the layers in the `forward()` method.\n\n\n3. **Training**:\n    - Select an appropriate loss function (`nn.CrossEntropyLoss`, `nn.MSELoss`, etc.).\n    - Choose an optimizer (`optim.Adam`, `optim.SGD`, etc.).\n    - **Training Loop** -- For each batch:\n        1. Pass input data through your model to produce predictions (logits).\n        2. Compute the loss w.r.t. the gold label (target).\n        3. Perform backpropagation (`loss.backward()`) to calculate gradients of all model parameters.\n        4. Update the model parameters with your optimizer (`optimizer.step()`).\n        5. Reset gradients (`optimizer.zero_grad()`).\n\nThis structured workflow helps streamline model development and makes training neural networks clear and efficient.\n\nThis notebook will walk you through this process - you need to learn it and the complete the missing code segment marked with a `#TODO` comment.","metadata":{"id":"FmlLBejDCw6X"}},{"id":"IMXO6OkGIU1j","cell_type":"markdown","source":"## ğŸ”¹ Step 0: One-time Preparations\n\n### Step 0.1: Install Python Dependencies","metadata":{"id":"IMXO6OkGIU1j"}},{"id":"hKyvKyjaIgWA","cell_type":"code","source":"%pip install torch pandas scikit-learn","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hKyvKyjaIgWA","outputId":"219975a9-d3b6-4792-b691-4e9396ee1ffd","trusted":true,"execution":{"iopub.status.busy":"2025-04-27T13:02:57.945104Z","iopub.execute_input":"2025-04-27T13:02:57.945484Z","iopub.status.idle":"2025-04-27T13:04:25.554072Z","shell.execute_reply.started":"2025-04-27T13:02:57.945465Z","shell.execute_reply":"2025-04-27T13:04:25.553324Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.2->pandas) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.2->pandas) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.2->pandas) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":5},{"id":"c7573a39","cell_type":"markdown","source":"### Step 0.2: Download the Dataset\nDownload the provided dataset file (`train.csv`) from the following link:\n\n[train.csv](https://drive.google.com/file/d/1eHBj_mdKjPfj_NuXy0zCG5IkQMJLGpPM/view?usp=sharing)\n\nThen upload the file to your Colab notebook or Jupyter environment.","metadata":{"id":"c7573a39"}},{"id":"8d22c22e","cell_type":"markdown","source":"## ğŸ”¹ Step 1: Prepare Data","metadata":{"id":"8d22c22e"}},{"id":"a70075c2","cell_type":"code","source":"import pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport random\nfrom sklearn.model_selection import train_test_split\n\nrandom_seed = 123\nrandom.seed(random_seed)\ntorch.manual_seed(random_seed)\n\ndf = pd.read_csv('/kaggle/input/train-df/train.csv')\ndf.head()","metadata":{"id":"a70075c2","colab":{"base_uri":"https://localhost:8080/","height":206},"outputId":"aefa2069-a3b2-4c92-a109-e7cc752ba523","trusted":true,"execution":{"iopub.status.busy":"2025-05-06T08:50:04.276587Z","iopub.execute_input":"2025-05-06T08:50:04.276831Z","iopub.status.idle":"2025-05-06T08:50:08.761689Z","shell.execute_reply.started":"2025-05-06T08:50:04.276812Z","shell.execute_reply":"2025-05-06T08:50:08.760906Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0                                     input_sentence  \\\n0           0           to think need we about. That's something   \n1           1          is mountains. the up moon coming over The   \n2           2  committee. the The through Congressmen bill ra...   \n3           3            careful late I'll to never again. be be   \n4           4           please.\" gifts, \"No The said, invitation   \n\n                                     target_sentence  \n0           That's something we need to think about.  \n1          The moon is coming up over the mountains.  \n2  The Congressmen rammed the bill through commit...  \n3            I'll be careful never to be late again.  \n4           The invitation said, \"No gifts, please.\"  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>input_sentence</th>\n      <th>target_sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>to think need we about. That's something</td>\n      <td>That's something we need to think about.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>is mountains. the up moon coming over The</td>\n      <td>The moon is coming up over the mountains.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>committee. the The through Congressmen bill ra...</td>\n      <td>The Congressmen rammed the bill through commit...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>careful late I'll to never again. be be</td>\n      <td>I'll be careful never to be late again.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>please.\" gifts, \"No The said, invitation</td>\n      <td>The invitation said, \"No gifts, please.\"</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"id":"c21b2edd","cell_type":"code","source":"train_df, dev_df = train_test_split(df, test_size=0.1, random_state=42)\nprint(f\"Training examples: {len(train_df)}\")\nprint(f\"Development examples: {len(dev_df)}\")","metadata":{"id":"c21b2edd","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ca8d3247-fce5-48d0-d1e4-4971a52f4f0e","trusted":true,"execution":{"iopub.status.busy":"2025-05-06T08:50:08.762786Z","iopub.execute_input":"2025-05-06T08:50:08.763042Z","iopub.status.idle":"2025-05-06T08:50:08.775059Z","shell.execute_reply.started":"2025-05-06T08:50:08.763023Z","shell.execute_reply":"2025-05-06T08:50:08.774356Z"}},"outputs":[{"name":"stdout","text":"Training examples: 31500\nDevelopment examples: 3500\n","output_type":"stream"}],"execution_count":3},{"id":"JRGMPu8eKVnI","cell_type":"markdown","source":"#### Pedagogical Note: **Why do we split train data into training and development (dev) sets?**\n\nWhen developing machine learning models, we want to ensure our model not only performs well on the data it has seen during training but also generalizes effectively to **new, unseen data**.\n\n- **Training set**:  \n  Used by the model to learn patterns. This is the data your model sees repeatedly during the training process.\n\n- **Development (dev) set** *(also known as validation set)*:  \n  Used to evaluate the model's performance on unseen examples during training. By checking the model periodically against the dev set, we can identify and prevent **overfitting**â€”when the model performs excellently on training data but poorly on new examples.\n\nThus, by splitting the data, we ensure our model truly learns generalizable patterns rather than memorizing the specific examples it trained on.\n","metadata":{"id":"JRGMPu8eKVnI"}},{"id":"9df6d543","cell_type":"code","source":"# Handle Vocabulary and Tokenization\nfrom collections import Counter\n\ndef tokenize(sentence):\n    return sentence.lower().split()\n\ncounter = Counter()\nfor sentence in train_df['input_sentence']:\n    counter.update(tokenize(sentence))\n\n# Special Tokens\nPAD = '<PAD>'\nSOS = '<SOS>'\nEOS = '<EOS>'\nUNK = '<UNK>'   # to handle out-of-vocabulary words\n\nwords = [PAD, SOS, EOS, UNK] + [w for w, c in counter.items() if c >= 1]\n# we maintain mapping between words (vocabulary entries) and their ids\nword2idx = {w: i for i, w in enumerate(words)}\nidx2word = {i: w for w, i in word2idx.items()}\n\nvocab_size = len(word2idx)\nprint(f\"Vocabulary size: {vocab_size}\")\n\ndef encode_sentence(sentence: str, word2idx, max_len):\n    # Replace sentence string with a fixed-length list of ints (token_ids with padding)\n    tokens = tokenize(sentence)\n    token_ids = [word2idx.get(w, word2idx[UNK]) for w in tokens]\n    token_ids = token_ids[:max_len-1]\n    token_ids.append(word2idx[EOS])\n    padding = [word2idx[PAD]] * (max_len - len(token_ids))\n    return token_ids + padding","metadata":{"id":"9df6d543","colab":{"base_uri":"https://localhost:8080/"},"outputId":"759df90d-c345-40f5-8935-bc17bdcfcdaa","trusted":true,"execution":{"iopub.status.busy":"2025-05-06T08:50:10.135829Z","iopub.execute_input":"2025-05-06T08:50:10.136145Z","iopub.status.idle":"2025-05-06T08:50:10.212663Z","shell.execute_reply.started":"2025-05-06T08:50:10.136118Z","shell.execute_reply":"2025-05-06T08:50:10.212060Z"}},"outputs":[{"name":"stdout","text":"Vocabulary size: 17063\n","output_type":"stream"}],"execution_count":4},{"id":"9a1b859c","cell_type":"code","source":"# Custom Dataset class for sentence-to-sentence mapping\nclass SentenceDataset(torch.utils.data.Dataset):\n    def __init__(self, df, word2idx, max_len):\n        # Store input and target sentences as lists of strings\n        self.input_sentences = df['input_sentence'].tolist()\n        self.target_sentences = df['target_sentence'].tolist()\n        self.word2idx = word2idx  # mapping from word to index\n        self.max_len = max_len    # maximum length of sentence (for padding)\n\n    def __len__(self):\n        # Return number of examples in the dataset\n        return len(self.input_sentences)\n\n    def __getitem__(self, idx):\n        # Encode both input and target sentences to fixed-length tensors of token IDs\n        src = encode_sentence(self.input_sentences[idx], self.word2idx, self.max_len)\n        trg = encode_sentence(self.target_sentences[idx], self.word2idx, self.max_len)\n        return torch.tensor(src), torch.tensor(trg)  # return as PyTorch tensors\n\n# Define batch size and maximum sentence length\nbatch_size = 32\nmax_len = 50  # All sequences will be padded/truncated to this length\n\n# Create training dataset and dataloader\ntrain_dataset = SentenceDataset(train_df, word2idx, max_len)\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True\n)\n# DataLoader loads batches from the dataset and optionally shuffles them\n\n# Create development (validation) dataset and dataloader\ndev_dataset = SentenceDataset(dev_df, word2idx, max_len)\ndev_loader = torch.utils.data.DataLoader(\n    dev_dataset, batch_size=batch_size\n)\n","metadata":{"id":"9a1b859c","trusted":true,"execution":{"iopub.status.busy":"2025-05-06T08:50:12.796059Z","iopub.execute_input":"2025-05-06T08:50:12.796601Z","iopub.status.idle":"2025-05-06T08:50:12.805988Z","shell.execute_reply.started":"2025-05-06T08:50:12.796577Z","shell.execute_reply":"2025-05-06T08:50:12.805300Z"}},"outputs":[],"execution_count":5},{"id":"Wm8yYkpes3u3","cell_type":"code","source":"# Get one batch of (src, trg) from the DataLoader\nsrc_batch, trg_batch = next(iter(train_loader))\n\n# Print their shapes\nprint(\"src_batch.shape =\", src_batch.shape)\nprint(\"trg_batch.shape =\", trg_batch.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wm8yYkpes3u3","outputId":"511f32d7-1eef-4fcf-8f76-3c175741bb9b","trusted":true,"execution":{"iopub.status.busy":"2025-05-06T08:50:14.559528Z","iopub.execute_input":"2025-05-06T08:50:14.559797Z","iopub.status.idle":"2025-05-06T08:50:14.614278Z","shell.execute_reply.started":"2025-05-06T08:50:14.559770Z","shell.execute_reply":"2025-05-06T08:50:14.613666Z"}},"outputs":[{"name":"stdout","text":"src_batch.shape = torch.Size([32, 50])\ntrg_batch.shape = torch.Size([32, 50])\n","output_type":"stream"}],"execution_count":6},{"id":"7o3Mz8zr3foD","cell_type":"markdown","source":"## PyTorch Quick Reference\n\nSince this is your first encounter with PyTorch in the course, here's a short summary of essential PyTorch classes used in this notebook:\n","metadata":{"id":"7o3Mz8zr3foD"}},{"id":"hKLOmTr0BHyk","cell_type":"markdown","source":"\n---\n### âœ… torch.utils.data.Dataset\n#### What is it?\n\nAn abstract class representing your dataset. It lets you define exactly how to access and prepare each data point.\n\n#### How to use it?\n\nYou subclass it and implement two methods:\n\n`__len__(self)`: returns the size of your dataset.\n\n`__getitem__(self, index)`: returns one data point (input-target pair).\n\n#### Why do we use it?\nIt provides a clean way to structure your data and feed it systematically into your model.\n\n---\n### âœ… torch.utils.data.DataLoader\n#### What is it?\nA utility that takes a Dataset object and provides an iterator over it.\n\n#### How to use it?\nSpecify batch size, shuffle options, and more:\n\n```python\nloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n```\n#### Why do we use it?\nIt handles batching, shuffling, and efficient parallel data loading automaticallyâ€”making your training loop concise and efficient.\n\n---\n### âœ… torch.nn.Module\n#### What is it?\nThe base class for all neural network models in PyTorch. Every model you build inherits from it.\n\n#### How to use it?\nYou subclass it, define your layers in the constructor (`__init__`), and specify the forward pass in the `forward()` method:\n\n```python\nclass MyModel(nn.Module):\n    def __init__(self):\n        super(MyModel, self).__init__()\n        self.linear = nn.Linear(10, 1)\n    \n    def forward(self, x):\n        return self.linear(x)\n```\n#### Why do we use it?\nIt manages model parameters, handles the forward computation, and simplifies tasks such as moving models to GPU or tracking gradients automatically.\n\n\n","metadata":{"id":"hKLOmTr0BHyk"}},{"id":"VlTEAM4y3FA0","cell_type":"markdown","source":"## ğŸ”¹ Step 2: Define Model","metadata":{"id":"VlTEAM4y3FA0"}},{"id":"DuXH8zUee6Ud","cell_type":"markdown","source":"### Building blocks - Enocder and Decoder","metadata":{"id":"DuXH8zUee6Ud"}},{"id":"6c990ab2","cell_type":"code","source":"class EncoderRNN(nn.Module):\n    def __init__(self, vocab_size, embedding_size, hidden_size, num_layers=2): # Added num_layers argument\n        super(EncoderRNN, self).__init__()\n        self.num_layers = num_layers\n        self.hidden_size = hidden_size # Store hidden_size\n        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size)\n\n        # --- CHANGE: Make LSTM Bidirectional ---\n        self.lstm = nn.LSTM(input_size=embedding_size,\n                            hidden_size=hidden_size,\n                            num_layers=num_layers,\n                            batch_first=False, # Keep False: input is (seq_len, batch)\n                            bidirectional=True) # Enable bidirectional\n\n    def forward(self, input, hidden):\n        \"\"\"\n        input: (seq_len, batch_size)\n        hidden: tuple (h_0, c_0), each (num_layers * 2, batch_size, hidden_size)\n        \"\"\"\n        embedding = self.embedding(input)\n        # embedding shape: (seq_len, batch_size, embedding_size)\n\n        # Pass embedding and initial hidden state (must match num_layers * 2)\n        outputs, hidden = self.lstm(embedding, hidden)\n        # outputs shape: (seq_len, batch_size, hidden_size * 2) -> Contains outputs from both directions\n        # hidden tuple (h_n, c_n) shapes: (num_layers * 2, batch_size, hidden_size)\n        return outputs, hidden\n\n    def init_hidden(self, batch_size):\n        \"\"\" Initialize hidden state for bidirectional LSTM \"\"\"\n        device = next(self.parameters()).device # Get device from model parameters\n        # --- CHANGE: Double the first dimension for bidirectional ---\n        h_0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size, device=device)\n        c_0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size, device=device)\n        return (h_0, c_0)\n\n# In DecoderRNN:\nclass DecoderRNN(nn.Module):\n    # Using default num_layers=2 to match common setup\n    def __init__(self, vocab_size, embedding_size, hidden_size, num_layers=2, dropout_p=0.1): # Added dropout\n        super(DecoderRNN, self).__init__()\n        self.num_layers = num_layers\n        self.hidden_size = hidden_size\n        self.embedding = nn.Embedding(vocab_size, embedding_size)\n        self.embedding_dropout = nn.Dropout(dropout_p) # Add dropout\n        # LSTM expects input shape: (batch, seq_len=1, embedding_size)\n        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers,\n                            batch_first=True,\n                            dropout=dropout_p if num_layers > 1 else 0) # LSTM dropout\n        self.out = nn.Linear(hidden_size, vocab_size)\n\n    def forward(self, input_token, hidden_state):\n        # input_token: (batch_size)\n        # hidden_state: tuple (h_prev, c_prev), each (num_layers, batch_size, hidden_size)\n\n        embedded = self.embedding(input_token) # (batch, embedding_dim)\n        embedded = self.embedding_dropout(embedded)\n        embedded = embedded.unsqueeze(1) # (batch, 1, embedding_dim)\n\n        lstm_out, hidden = self.lstm(embedded, hidden_state)\n        # lstm_out: (batch, 1, hidden_size)\n\n        lstm_out_squeezed = lstm_out.squeeze(1) # (batch, hidden_size)\n        output_logits = self.out(lstm_out_squeezed) # (batch, vocab_size)\n\n        return output_logits, hidden","metadata":{"id":"6c990ab2","trusted":true,"execution":{"iopub.status.busy":"2025-05-06T08:53:25.168453Z","iopub.execute_input":"2025-05-06T08:53:25.168758Z","iopub.status.idle":"2025-05-06T08:53:25.178767Z","shell.execute_reply.started":"2025-05-06T08:53:25.168735Z","shell.execute_reply":"2025-05-06T08:53:25.178002Z"}},"outputs":[],"execution_count":8},{"id":"08f70b0d","cell_type":"code","source":"# Define hyperparameters for the check (use dimensions matching your trained models if possible)\nembedding_size_check = 128 # Match attention model\nhidden_size_check = 256  # Match attention model\nnum_layers_check = 2\n\n# Ensure vocab_size, word2idx, SOS are defined\n# Instantiate the Bidirectional Encoder\nencoder = EncoderRNN(vocab_size, embedding_size_check, hidden_size_check, num_layers=num_layers_check)\n# Instantiate the BASIC Unidirectional Decoder\ndecoder = DecoderRNN(vocab_size, embedding_size_check, hidden_size_check, num_layers=num_layers_check)\n\n# Move models to device\ndevice_check = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nencoder.to(device_check)\ndecoder.to(device_check)\n\n# Get a sample batch\nsample_input, sample_target = next(iter(train_loader))\nsample_input, sample_target = sample_input.to(device_check), sample_target.to(device_check)\nbatch_size_check = sample_input.size(0)\n\nprint(f\"Sample input shape (Batch First): {sample_input.shape}\")\n\n# Initialize Bidirectional hidden state\nencoder_hidden_init = encoder.init_hidden(batch_size_check)\n\n# Transpose input for the encoder\nsample_input_transposed = sample_input.T\nprint(f\"Transposed input shape for Encoder: {sample_input_transposed.shape}\")\n\n# Call the Bidirectional encoder\nencoder_outputs, encoder_final_hidden = encoder(sample_input_transposed, encoder_hidden_init)\n# encoder_final_hidden is tuple (h_n, c_n) with h_n shape (4, 32, 256)\n\n# --- Adapt Hidden State for Basic Decoder ---\nh_n_enc, c_n_enc = encoder_final_hidden\n# Reshape to (num_layers, 2, batch, hidden_size)\nh_n_enc_reshaped = h_n_enc.view(encoder.num_layers, 2, batch_size_check, encoder.hidden_size)\nc_n_enc_reshaped = c_n_enc.view(encoder.num_layers, 2, batch_size_check, encoder.hidden_size)\n# Select forward states: (num_layers, batch, hidden_size) -> (2, 32, 256)\ndecoder_h_init = h_n_enc_reshaped[:, 0, :, :].contiguous()\ndecoder_c_init = c_n_enc_reshaped[:, 0, :, :].contiguous()\ndecoder_hidden = (decoder_h_init, decoder_c_init)\n# --- End Adaptation ---\n\n# Initialize basic decoder input\ndecoder_input = torch.tensor([word2idx[SOS]] * batch_size_check, device=device_check)\n\n# Call the basic decoder with the *adapted* hidden state\ndecoder_output_logits, decoder_hidden_updated = decoder(decoder_input, decoder_hidden)\n\nprint(\"\\n--- Sanity Check Results (Bi-Encoder -> Basic Decoder) ---\")\nprint(\"Encoder outputs shape:\", encoder_outputs.shape) # Should be [50, 32, 512]\nprint(\"Encoder final h_n shape:\", encoder_final_hidden[0].shape) # Should be [4, 32, 256]\nprint(\"Adapted initial h_n for Decoder shape:\", decoder_hidden[0].shape) # Should be [2, 32, 256]\nprint(\"Decoder output logits shape:\", decoder_output_logits.shape) # Should be [32, vocab_size]\nprint(\"Sanity Check Passed!\")","metadata":{"id":"08f70b0d","colab":{"base_uri":"https://localhost:8080/"},"outputId":"09bbbd84-a233-45d3-c65b-8a866f74f68c","trusted":true,"execution":{"iopub.status.busy":"2025-05-06T08:53:26.933042Z","iopub.execute_input":"2025-05-06T08:53:26.933361Z","iopub.status.idle":"2025-05-06T08:53:27.705010Z","shell.execute_reply.started":"2025-05-06T08:53:26.933341Z","shell.execute_reply":"2025-05-06T08:53:27.704247Z"}},"outputs":[{"name":"stdout","text":"Sample input shape (Batch First): torch.Size([32, 50])\nTransposed input shape for Encoder: torch.Size([50, 32])\n\n--- Sanity Check Results (Bi-Encoder -> Basic Decoder) ---\nEncoder outputs shape: torch.Size([50, 32, 512])\nEncoder final h_n shape: torch.Size([4, 32, 256])\nAdapted initial h_n for Decoder shape: torch.Size([2, 32, 256])\nDecoder output logits shape: torch.Size([32, 17063])\nSanity Check Passed!\n","output_type":"stream"}],"execution_count":9},{"id":"jQLknjFt-5bg","cell_type":"markdown","source":"### Seq2Seq model that wraps Encoder and Decoder together","metadata":{"id":"jQLknjFt-5bg"}},{"id":"JdusuF9w_Ea4","cell_type":"code","source":"# MODIFIED Seq2Seq Class Definition\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, sos_idx, device):\n        super(Seq2Seq, self).__init__()\n        # Encoder IS the Bidirectional one\n        self.encoder = encoder\n        # Decoder IS the Basic Unidirectional one (without context)\n        self.decoder = decoder\n        self.sos_idx = sos_idx\n        self.device = device\n        # Optional: Add a check for encoder bidirectionality if desired\n        if not self.encoder.lstm.bidirectional:\n            print(\"Warning: Basic Seq2Seq model expects a bidirectional encoder for hidden state adaptation.\")\n        if hasattr(self.decoder, 'attn_combine'): # Crude check for attention decoder\n             print(\"Warning: Basic Seq2Seq model initialized with an Attention Decoder!\")\n\n\n    def forward(self, src, trg):\n        batch_size, trg_len = trg.shape\n        vocab_size = self.decoder.out.out_features\n        outputs = torch.zeros(batch_size, trg_len, vocab_size, device=self.device)\n\n        src = src.T.to(self.device) # Transpose for Encoder (expects seq_len, batch)\n        trg = trg.T.to(self.device)\n\n        # Initialize Bidirectional Encoder hidden state\n        encoder_hidden_init = self.encoder.init_hidden(batch_size)\n\n        # Encode the input sentence\n        # encoder_outputs shape: (seq_len, batch, enc_hidden * 2)\n        # encoder_final_hidden tuple (h_n, c_n): each (enc_layers * 2, batch, enc_hidden)\n        encoder_outputs, encoder_final_hidden = self.encoder(src, encoder_hidden_init)\n\n        # --- Adapt Bidirectional Encoder Hidden State for Unidirectional Decoder ---\n        # h_n shape: (num_layers * 2, batch, hidden_size) -> e.g., (4, batch, 256)\n        # c_n shape: (num_layers * 2, batch, hidden_size) -> e.g., (4, batch, 256)\n        h_n, c_n = encoder_final_hidden\n\n        # Reshape to separate layers and directions: (num_layers, 2, batch, hidden_size)\n        h_n_reshaped = h_n.view(self.encoder.num_layers, 2, batch_size, self.encoder.hidden_size)\n        c_n_reshaped = c_n.view(self.encoder.num_layers, 2, batch_size, self.encoder.hidden_size)\n\n        # Select only the forward direction hidden states for each layer\n        # Shape becomes (num_layers, batch, hidden_size) -> e.g., (2, batch, 256)\n        decoder_hidden_h = h_n_reshaped[:, 0, :, :].contiguous()\n        decoder_hidden_c = c_n_reshaped[:, 0, :, :].contiguous()\n\n        # This is the initial hidden state for the basic decoder\n        decoder_hidden = (decoder_hidden_h, decoder_hidden_c)\n        # --- End Hidden State Adaptation ---\n\n        # Start decoding with the <SOS> token\n        input_token = torch.full((batch_size,), self.sos_idx, dtype=torch.long, device=self.device)\n\n        for t in range(trg_len):\n            # Call the basic decoder, passing the adapted hidden state\n            output, decoder_hidden = self.decoder(input_token, decoder_hidden) # No context\n            outputs[:, t, :] = output\n            input_token = trg[t]  # Teacher forcing\n\n        return outputs","metadata":{"id":"JdusuF9w_Ea4","trusted":true,"execution":{"iopub.status.busy":"2025-05-06T08:53:30.463609Z","iopub.execute_input":"2025-05-06T08:53:30.463892Z","iopub.status.idle":"2025-05-06T08:53:30.471710Z","shell.execute_reply.started":"2025-05-06T08:53:30.463869Z","shell.execute_reply":"2025-05-06T08:53:30.470964Z"}},"outputs":[],"execution_count":10},{"id":"WAmJH-5feIkb","cell_type":"markdown","source":"## ğŸ”¹ Step 3: Training","metadata":{"id":"WAmJH-5feIkb"}},{"id":"0lUGBZ7pd0Rw","cell_type":"code","source":"def train_model(model, train_loader, dev_loader, optimizer, criterion, device,\n                word2idx, idx2word, max_len, # Need these for accuracy check\n                num_epochs=10, clip=1.0, patience=5,\n                model_path_best_acc='best_acc_basic_model.pt'): # Path for best accuracy model\n\n    # --- Assertions to Ensure Correct Model Type ---\n    # Check if the encoder passed is likely the unidirectional one for basic\n    # (If you strictly use BiEncoder for both, this check needs adjustment/removal)\n    # if model.encoder.lstm.bidirectional:\n    #    print(\"Warning: Training basic model with a Bidirectional encoder via Seq2Seq wrapper.\")\n    # Check if decoder looks like the basic one (no context handling attributes)\n    if hasattr(model.decoder, 'attn_combine') or hasattr(model.decoder, 'fc_context'):\n         raise TypeError(\"The train_model function (basic version) received a model with an Attention Decoder.\")\n    # -----------------------------------------------\n\n    model.to(device)\n    train_losses = []\n    dev_losses = []\n    dev_accuracies = [] # Track dev accuracy\n\n    best_dev_loss = float('inf') # Needed for patience calculation\n    best_dev_acc = -1.0 # Start accuracy at -1\n    patience_counter = 0\n    epochs_ran = 0\n\n    # Define dev sentences here for efficiency\n    # Make sure dev_df is accessible in the scope where train_model is called\n    dev_shuffled_sentences = dev_df['input_sentence'].tolist()\n    dev_target_sentences = dev_df['target_sentence'].tolist()\n\n    print(f\"\\nStarting training for up to {num_epochs} epochs...\")\n    print(f\"Early stopping patience (on loss): {patience} epochs.\")\n    print(f\"Saving model with best DEV ACCURACY to: {model_path_best_acc}\")\n    if clip > 0: print(f\"Using gradient clipping with max_norm: {clip}\")\n\n    # Ensure the basic inference function is defined in the global scope before calling train_model\n    # infer_sentences_basic_simple should be the correct one here\n    if 'infer_sentences' not in globals():\n        raise NameError(\"Function 'infer_sentences_basic_simple' is not defined. Please define it before calling train_model.\")\n\n    for epoch in range(num_epochs):\n        epochs_ran += 1\n        model.train()\n        epoch_train_loss = 0\n        # --- Training Loop ---\n        for src, trg in train_loader:\n            src, trg = src.to(device), trg.to(device)\n            optimizer.zero_grad()\n            output = model(src, trg)\n            output_flat = output.view(-1, output.shape[-1])\n            trg_flat = trg.view(-1)\n            loss = criterion(output_flat, trg_flat)\n            loss.backward()\n            if clip > 0: torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n            optimizer.step()\n            epoch_train_loss += loss.item()\n        avg_train_loss = epoch_train_loss / len(train_loader)\n        train_losses.append(avg_train_loss)\n\n        # --- Evaluation Loop ---\n        model.eval()\n        epoch_dev_loss = 0\n        with torch.no_grad():\n            # Calculate Dev Loss\n            for src, trg in dev_loader:\n                src, trg = src.to(device), trg.to(device)\n                output = model(src, trg)\n                output_flat = output.view(-1, output.shape[-1])\n                trg_flat = trg.view(-1)\n                loss = criterion(output_flat, trg_flat)\n                epoch_dev_loss += loss.item()\n            avg_dev_loss = epoch_dev_loss / len(dev_loader)\n            dev_losses.append(avg_dev_loss)\n\n            # Calculate Dev Accuracy using the BASIC inference function\n            print(f\"\\nEpoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Dev Loss: {avg_dev_loss:.4f}\", end=' | ')\n            print(\"Running dev accuracy check...\", end='')\n            # Ensure evaluate_sentence_predictions (with .lower()) is defined globally\n            # Directly call the basic inference function (greedy)\n            current_predictions = infer_sentences(dev_shuffled_sentences, model, word2idx, idx2word, device, max_len)\n            current_dev_acc = evaluate_sentence_predictions(current_predictions, dev_target_sentences)\n            dev_accuracies.append(current_dev_acc)\n            print(f\"Dev Acc: {current_dev_acc:.2f}%\", end='')\n\n        # --- Checkpoints and Early Stopping ---\n        # Save based on best ACCURACY\n        if current_dev_acc > best_dev_acc:\n            best_dev_acc = current_dev_acc\n            if model_path_best_acc:\n                torch.save(model.state_dict(), model_path_best_acc)\n                print(\" | Best Dev Acc Saved\", end='')\n\n        # Early stopping based on LOSS improvement\n        if avg_dev_loss < best_dev_loss:\n            best_dev_loss = avg_dev_loss\n            patience_counter = 0\n            print(\" | Dev Loss Improved\", end='') # Indicate loss improvement\n        else:\n            patience_counter += 1\n            print(f\" | Patience Counter (Loss): {patience_counter}/{patience}\", end='')\n            if patience_counter >= patience:\n                print(\"\\nEarly stopping triggered (based on dev loss).\")\n                break\n\n        print() # Newline for next epoch\n\n    print(f\"\\nTraining finished after {epochs_ran} epochs.\")\n    print(f\"Best Dev Loss achieved: {min(dev_losses):.4f}\")\n    # Handle case where dev_accuracies might be empty if training stopped early\n    if dev_accuracies:\n        print(f\"Best Dev Accuracy achieved: {max(dev_accuracies):.2f}% (Saved to {model_path_best_acc})\")\n    else:\n        print(\"No Dev Accuracy recorded.\")\n\n    # Return all tracked metrics\n    return train_losses, dev_losses, dev_accuracies","metadata":{"id":"0lUGBZ7pd0Rw","trusted":true,"execution":{"iopub.status.busy":"2025-05-06T08:56:34.793491Z","iopub.execute_input":"2025-05-06T08:56:34.794183Z","iopub.status.idle":"2025-05-06T08:56:34.805789Z","shell.execute_reply.started":"2025-05-06T08:56:34.794159Z","shell.execute_reply":"2025-05-06T08:56:34.805048Z"}},"outputs":[],"execution_count":20},{"id":"AqiWK2_-fOWF","cell_type":"code","source":"# Hyperparameters\nembedding_size_basic = 128\nhidden_size_basic = 256\nnum_layers_basic = 2\nlr_basic = 0.001\nnum_epochs_basic_max = 15 # MAX epochs\nclip_basic = 1.0\nweight_decay_basic = 1e-5\npatience_basic = 5\nbasic_model_acc_save_path = 'best_acc_basic_model.pt' # Path for best accuracy model\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\npad_idx = word2idx[PAD]\nsos_idx = word2idx[SOS]\n\n# Ensure these are defined: vocab_size, word2idx, PAD, SOS, device, train_loader, dev_loader, dev_df, max_len\n# Ensure EncoderRNN (Bi) and basic DecoderRNN (no context) definitions are active/correct before this cell\n\n# Instantiate Model Components\nencoder_for_basic = EncoderRNN(vocab_size, embedding_size_basic, hidden_size_basic, num_layers=num_layers_basic)\ndecoder_for_basic = DecoderRNN(vocab_size, embedding_size_basic, hidden_size_basic, num_layers=num_layers_basic, dropout_p=0.3) # Use basic decoder\n\n# Instantiate the Seq2Seq model (handles hidden state adaptation if encoder is Bi)\nmodel_basic = Seq2Seq(encoder_for_basic, decoder_for_basic, sos_idx, device)\nmodel_basic.to(device)\n\nprint(\"\\n--- Training Basic Seq2Seq Model (Tracking Best Accuracy) ---\")\nprint(f\"Model Parameters: {sum(p.numel() for p in model_basic.parameters() if p.requires_grad):,}\")\n\n# Define Optimizer and Criterion\noptimizer_basic = optim.Adam(model_basic.parameters(), lr=lr_basic, weight_decay=weight_decay_basic)\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n\n# Train using the FINAL SIMPLIFIED train_model function\ntrain_losses_basic, dev_losses_basic, dev_accs_basic = train_model( # Capture accuracy list\n    model=model_basic,\n    train_loader=train_loader,\n    dev_loader=dev_loader,\n    optimizer=optimizer_basic,\n    criterion=criterion,\n    device=device,\n    word2idx=word2idx,             # Pass for inference check\n    idx2word=idx2word,             # Pass for inference check\n    max_len=max_len,               # Pass for inference check\n    num_epochs=num_epochs_basic_max,\n    clip=clip_basic,\n    patience=patience_basic,\n    model_path_best_acc=basic_model_acc_save_path # Save best acc model\n    # NO beam_width_eval argument needed or passed\n)\n\n# Plot losses\nprint(\"\\nPlotting basic model losses...\")\nplot_losses(train_losses_basic, dev_losses_basic) # Ensure plot_losses exists\n\n# --- Optional: Report Best Accuracy Found during training ---\nbest_epoch_acc = np.argmax(dev_accs_basic) if dev_accs_basic else -1\nif best_epoch_acc != -1:\n    print(f\"\\nBest Dev Accuracy ({max(dev_accs_basic):.2f}%) achieved at Epoch {best_epoch_acc + 1}\")\nelse:\n    print(\"\\nNo Dev Accuracy recorded.\")\n\nprint(\"--- Basic Model Training Finished ---\")","metadata":{"id":"AqiWK2_-fOWF","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d6553317-0778-4e58-dfa9-ea18fb9ad3e3","trusted":true,"execution":{"iopub.status.busy":"2025-05-06T08:56:58.845318Z","iopub.execute_input":"2025-05-06T08:56:58.845789Z","iopub.status.idle":"2025-05-06T08:59:10.782848Z","shell.execute_reply.started":"2025-05-06T08:56:58.845766Z","shell.execute_reply":"2025-05-06T08:59:10.781966Z"}},"outputs":[{"name":"stdout","text":"\n--- Training Basic Seq2Seq Model (Tracking Best Accuracy) ---\nModel Parameters: 12,042,407\n\nStarting training for up to 15 epochs...\nEarly stopping patience (on loss): 5 epochs.\nSaving model with best DEV ACCURACY to: best_acc_basic_model.pt\nUsing gradient clipping with max_norm: 1.0\n\nEpoch 1/15 | Train Loss: 5.7543 | Dev Loss: 5.1580 | Running dev accuracy check...Exact match accuracy: 0.00%\nDev Acc: 0.00% | Best Dev Acc Saved | Dev Loss Improved\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/853418093.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Train using the FINAL SIMPLIFIED train_model function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m train_losses_basic, dev_losses_basic, dev_accs_basic = train_model( # Capture accuracy list\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_basic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/1226892497.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, dev_loader, optimizer, criterion, device, word2idx, idx2word, max_len, num_epochs, clip, patience, model_path_best_acc)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mclip\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mepoch_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mavg_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_train_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_train_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":22},{"id":"k63dQnCoh3Zn","cell_type":"code","source":"# Helper - loss visualizations\nimport matplotlib.pyplot as plt\n\ndef plot_losses(train_losses, dev_losses):\n    plt.plot(train_losses, label='Train Loss')\n    plt.plot(dev_losses, label='Dev Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Training and Dev Loss over Epochs')\n    plt.legend()\n    plt.grid(True)\n    plt.show()","metadata":{"id":"k63dQnCoh3Zn","colab":{"base_uri":"https://localhost:8080/","height":472},"outputId":"854dacab-5f8b-4c07-f6be-d1b1a3884136","trusted":true,"execution":{"iopub.status.busy":"2025-05-06T08:56:50.059666Z","iopub.status.idle":"2025-05-06T08:56:50.059969Z","shell.execute_reply.started":"2025-05-06T08:56:50.059791Z","shell.execute_reply":"2025-05-06T08:56:50.059814Z"}},"outputs":[],"execution_count":null},{"id":"aO0t5R_HkEvF","cell_type":"markdown","source":"### Pedagogical Note: Interpreting the Loss Plots ğŸ”\n\nAfter training completes, you'll see two curves:\n\n- **Train Loss**: How well the model fits the training data.\n- **Dev Loss**: How well the model generalizes to unseen (validation) data.\n\nIdeally:\n- Both curves should **decrease over time**.\n- A small gap between them indicates **good generalization**.\n- If train loss decreases but dev loss increases, your model may be **overfitting**.\n\nUse these curves to decide if your model is learning stably and when to stop training.\n","metadata":{"id":"aO0t5R_HkEvF"}},{"id":"fSXqwoPDndhF","cell_type":"markdown","source":"## Step 4: Inference\nNow that you have a trained model, you can use it to solve your task on any new data points.\n\nComplete the code below for implementing a inference function for the model.","metadata":{"id":"fSXqwoPDndhF"}},{"id":"LOlykSQOoa_R","cell_type":"code","source":"def infer_sentences(shuffled_sentences, model, word2idx, idx2word, device, max_len=50):\n    \"\"\"\n    Generate predicted (unshuffled) sentences from a list of shuffled input sentences.\n    \"\"\"\n    model_basic.eval()\n    predictions = []\n\n    sos_idx = word2idx['<SOS>']\n    eos_idx = word2idx['<EOS>']\n    pad_idx = word2idx['<PAD>']\n    unk_idx = word2idx['<UNK>']\n\n    with torch.no_grad():\n        for sentence in shuffled_sentences:\n             # 1. Prepare Input\n            tokens = tokenize(sentence)\n            token_ids = [word2idx.get(w, unk_idx) for w in tokens]\n            current_len = len(token_ids)\n            token_ids = token_ids[:max_len] if current_len >= max_len else token_ids + [pad_idx] * (max_len - current_len)\n            src_tensor = torch.tensor([token_ids], dtype=torch.long, device=device)\n\n            # 2. Encode\n            src_tensor_enc = src_tensor.T\n            encoder_hidden_init = model.encoder.init_hidden(batch_size=1)\n            _, encoder_final_hidden = model.encoder(src_tensor_enc, encoder_hidden_init)\n\n            # 3. Adapt Hidden State\n            h_n_enc, c_n_enc = encoder_final_hidden\n            h_n_enc_reshaped = h_n_enc.view(model.encoder.num_layers, 2, 1, model.encoder.hidden_size)\n            c_n_enc_reshaped = c_n_enc.view(model.encoder.num_layers, 2, 1, model.encoder.hidden_size)\n            decoder_h_init = h_n_enc_reshaped[:, 0, :, :].contiguous()\n            decoder_c_init = c_n_enc_reshaped[:, 0, :, :].contiguous()\n            decoder_hidden = (decoder_h_init, decoder_c_init) # Initial state for basic decoder\n\n            # 4. Decode Loop\n            decoder_input = torch.tensor([sos_idx], device=device)\n            predicted_token_ids = []\n            for t in range(max_len):\n                output_logits, decoder_hidden = model.decoder(decoder_input, decoder_hidden) # Basic decoder call\n                predicted_token_id = output_logits.argmax(1).item()\n                if predicted_token_id == eos_idx: break\n                if predicted_token_id != pad_idx:\n                    predicted_token_ids.append(predicted_token_id)\n                else: break\n                decoder_input = torch.tensor([predicted_token_id], device=device)\n\n            # 5. Convert Output\n            predicted_words = [idx2word.get(idx, '<UNK>') for idx in predicted_token_ids]\n            predicted_sentence = \" \".join(predicted_words)\n            predictions.append(predicted_sentence)\n\n    return predictions","metadata":{"id":"LOlykSQOoa_R","trusted":true,"execution":{"iopub.status.busy":"2025-05-06T08:54:08.171709Z","iopub.execute_input":"2025-05-06T08:54:08.172335Z","iopub.status.idle":"2025-05-06T08:54:08.180240Z","shell.execute_reply.started":"2025-05-06T08:54:08.172302Z","shell.execute_reply":"2025-05-06T08:54:08.179414Z"}},"outputs":[],"execution_count":14},{"id":"Ddv1mOKWqA9p","cell_type":"markdown","source":"\nNow, use the above inference function to generate prediction for the **test set**. You are provided with the test set shuffled sentences in [this downloadable file](https://drive.google.com/file/d/178mEesTW89Ooz_f5bb6sHBHjAkG4DWMV/view?usp=sharing). In your submission, you should run the inference function of the provided list of shuffled sentences, to attain a `predicted_test.csv` file. Typically, these predictions over the test set will be evaluated against their annotated labels (targets).  \n\nIf you want to assess the quality of current trained model, you can run inference on the dev set and evaluate on it using the function below:","metadata":{"id":"Ddv1mOKWqA9p"}},{"id":"SE0shbcytAE1","cell_type":"code","source":"def evaluate_sentence_predictions(predictions, targets):\n    \"\"\"\n    Compute exact match accuracy between predicted and target sentences (as strings).\n    Args:\n        predictions (List[str])\n        targets (List[str])\n    Returns:\n        accuracy (float): percentage of exact string matches\n    \"\"\"\n    correct = 0\n    total = len(predictions)\n    for pred, true in zip(predictions, targets):\n        if pred.strip().lower() == true.strip().lower():\n            correct += 1\n    accuracy = correct / total * 100\n    print(f\"Exact match accuracy: {accuracy:.2f}%\")\n    return accuracy","metadata":{"id":"SE0shbcytAE1","trusted":true,"execution":{"iopub.status.busy":"2025-05-06T08:54:11.366498Z","iopub.execute_input":"2025-05-06T08:54:11.367077Z","iopub.status.idle":"2025-05-06T08:54:11.371448Z","shell.execute_reply.started":"2025-05-06T08:54:11.367052Z","shell.execute_reply":"2025-05-06T08:54:11.370632Z"}},"outputs":[],"execution_count":15},{"id":"0983f301-a42e-49a0-a297-3847845f97a1","cell_type":"code","source":"predictions_for_input = infer_sentences(dev_df['input_sentence'], model_basic, word2idx, idx2word, device)\nprint(\"Accuracy on dev:\")\naccuracy = evaluate_sentence_predictions(predictions_for_input, dev_df['target_sentence'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a7c63928-5752-46a9-b0cf-7eb3604054f3","cell_type":"code","source":"# Final Saving and Test Prediction Generation\n\n# Define paths\nkaggle_output_dir = '/kaggle/working/'\nbasic_model_save_path = 'best_basic_seq2seq_model.pt'\nbasic_pred_filename = 'seq2seq_predictions.csv'\nbasic_pred_path = os.path.join(kaggle_output_dir, basic_pred_filename)\ntest_set_path = '/kaggle/input/test-no-target/test_no_target.csv'\n\n# Basic Model Predictions\nprint(\"\\n--- Generating Test Predictions for BEST Basic Model ---\")\n\n# 1. Load Test Data\nif not os.path.exists(test_set_path):\n    print(f\"Error: Test set file '{test_set_path}' not found.\")\nelse:\n    test_df = pd.read_csv(test_set_path)\n    test_original_input_sentences = test_df['input_sentence'].tolist()\n\n    # 2. Instantiate Basic Model Architecture\n    emb_basic = 128 # Match the trained basic model's params\n    hid_basic = 256\n    layers_basic = 2\n    drop_basic = 0.3\n    encoder_load_basic = EncoderRNN(vocab_size, emb_basic, hid_basic, num_layers=layers_basic)\n    # Ensure this uses the BASIC DecoderRNN definition (no context)\n    decoder_load_basic = DecoderRNN(vocab_size, emb_basic, hid_basic, num_layers=layers_basic, dropout_p=drop_basic)\n    model_final_basic = Seq2Seq(encoder_load_basic, decoder_load_basic, sos_idx, device)\n\n    # 3. Load Best Basic Weights\n    if os.path.exists(basic_model_save_path):\n        print(f\"Loading best basic model weights from: {basic_model_save_path}\")\n        # Use map_location for broader compatibility, though 'cuda' should work if trained on GPU\n        model_final_basic.load_state_dict(torch.load(basic_model_save_path, map_location=device))\n        model_final_basic.to(device)\n        model_final_basic.eval()\n\n        # 4. Generate Predictions\n        print(\"Running inference on test set (Basic Model)...\")\n        basic_test_predictions = infer_sentences( # Use basic inference function\n            test_original_input_sentences,\n            model_final_basic, word2idx, idx2word, device, max_len\n        )\n        print(\"Basic model inference complete.\")\n\n        # 5. Save Predictions\n        print(f\"Saving basic model predictions to: {basic_pred_path}\")\n        basic_predictions_df = pd.DataFrame({\n            'input_sentence': test_original_input_sentences,\n            'predicted_sentence': basic_test_predictions\n        })\n        basic_predictions_df.to_csv(basic_pred_path, index=False)\n        print(\"Basic model predictions saved.\")\n    else:\n        print(f\"Error: Best basic model file '{basic_model_save_path}' not found.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"m9PRjqHkt50Z","cell_type":"markdown","source":"# ğŸ” Stage 2: Add Attention to Your Seq2Seq Model\n\nIn this stage, you'll implement a new model: **`Seq2SeqWithAttention`**.  \nThe goal is to improve your decoder by allowing it to \"attend\" to relevant parts of the encoder's outputs at each decoding step.\n\n### ğŸ§  Why Attention?\nThe encoder compresses the entire input sentence into a single hidden state. This limits performance, especially on long sentences.\n\nWith attention, the decoder **dynamically focuses on different encoder outputs**, depending on what it's trying to generate.\n\n---\n\n### ğŸ”§ What You Need to Implement:\nYou will create a new class `Seq2SeqWithAttention`, similar to `Seq2Seq`, but with the following differences:\n\n1. At each decoding step, compute **attention scores** over all encoder hidden states.\n2. Use those scores to compute a **context vector** (weighted sum of encoder outputs).\n3. Feed the context vector into the decoder along with the embedding of the current input token.\n\n---\n\n### ğŸ“Œ Tip: Dot-Product Attention\nA simple form of attention you can implement is dot-product attention:\n\n```python\nscore_t = dot(h_dec_t, h_enc_i) weights = softmax(score_t over i) context_t = sum_i weights[i] * h_enc_i\n```\n\nYou'll apply this at every decoder time step.\n","metadata":{"id":"m9PRjqHkt50Z"}},{"id":"7642e93a-379f-41e0-9da6-a561a25aec65","cell_type":"code","source":"class DecoderRNN(nn.Module):\n    def __init__(self, vocab_size, embedding_size, hidden_size, encoder_hidden_dim, num_layers=2, dropout_p=0.1): # Added encoder_hidden_dim, dropout_p\n        super(DecoderRNN, self).__init__()\n        self.num_layers = num_layers\n        self.hidden_size = hidden_size # Decoder's hidden size\n        self.encoder_hidden_dim = encoder_hidden_dim # Encoder's hidden size *per direction*\n\n        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size)\n        self.embedding_dropout = nn.Dropout(dropout_p) # Added embedding dropout\n\n        # --- Input to LSTM is JUST the embedding ---\n        # The context will be combined *after* the LSTM\n        self.lstm = nn.LSTM(input_size=embedding_size, # Input is just embedding\n                            hidden_size=hidden_size,\n                            num_layers=num_layers,\n                            batch_first=True,\n                            dropout=dropout_p if num_layers > 1 else 0) # Add LSTM dropout if multilayer\n\n        # --- CHANGE: Output layer combines LSTM output and RAW context ---\n        # Input features = decoder LSTM hidden size + encoder bidirectional hidden size (enc_hid*2)\n        self.out = nn.Linear(in_features=hidden_size + (encoder_hidden_dim * 2),\n                             out_features=vocab_size)\n\n    def forward(self, input_token, hidden, context):\n        \"\"\"\n        input_token: (batch_size)\n        hidden: tuple (h_prev, c_prev), each (num_layers, batch_size, hidden_size)\n        context: (batch_size, encoder_hidden_dim * 2) -> RAW attention context vector\n        \"\"\"\n        embedding = self.embedding(input_token) # (batch, embedding_size)\n        embedding = self.embedding_dropout(embedding) # Apply dropout\n\n        # Prepare for LSTM: Add sequence length dimension\n        embedding_unsqueezed = embedding.unsqueeze(1) # (batch, 1, embedding_size)\n\n        # Pass embedding and previous decoder hidden state to LSTM\n        lstm_out, hidden = self.lstm(embedding_unsqueezed, hidden)\n        # lstm_out shape: (batch, 1, hidden_size)\n        # hidden tuple shapes: (num_layers, batch, hidden_size)\n\n        lstm_out_squeezed = lstm_out.squeeze(1) # (batch, hidden_size)\n\n        # --- CHANGE: Concatenate LSTM output and RAW context vector ---\n        # lstm_out_squeezed: (batch, hidden_size)\n        # context:           (batch, encoder_hidden_dim * 2)\n        output_concat = torch.cat((lstm_out_squeezed, context), dim=1)\n        # output_concat shape: (batch, hidden_size + encoder_hidden_dim * 2)\n\n        # Apply final linear layer to the combined vector\n        output = self.out(output_concat) # (batch, vocab_size)\n\n        return output, hidden","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T23:08:35.558779Z","iopub.execute_input":"2025-04-30T23:08:35.559132Z","iopub.status.idle":"2025-04-30T23:08:35.567270Z","shell.execute_reply.started":"2025-04-30T23:08:35.559111Z","shell.execute_reply":"2025-04-30T23:08:35.566263Z"}},"outputs":[],"execution_count":12},{"id":"nPvAmc_euarf","cell_type":"code","source":"import math # Needed for sqrt\n\nclass Seq2SeqWithAttention(nn.Module):\n    def __init__(self, encoder, decoder, sos_idx, device):\n        super(Seq2SeqWithAttention, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.sos_idx = sos_idx\n        self.device = device\n\n        encoder_hidden_dim = self.encoder.hidden_size # Per direction\n        decoder_hidden_dim = self.decoder.hidden_size\n\n        # --- Layers to adapt BiEncoder hidden state to UniDecoder initial hidden state ---\n        self.fc_hidden = nn.Linear(encoder_hidden_dim * 2, decoder_hidden_dim)\n        self.fc_cell = nn.Linear(encoder_hidden_dim * 2, decoder_hidden_dim)\n\n        # --- Layers for Attention Calculation ---\n        # Layer to project encoder outputs (keys) to match decoder hidden dim (query) for scoring\n        self.attn_combine = nn.Linear(encoder_hidden_dim * 2, decoder_hidden_dim)\n        # fc_context IS REMOVED - we return the raw context vector now\n\n        # Optional: Add dropout layer\n        self.dropout = nn.Dropout(0.1) # Example dropout rate\n\n    def compute_attention(self, decoder_hidden_state, encoder_outputs):\n        \"\"\" Computes attention weights and the raw context vector. (SCALED DOT-PRODUCT) \"\"\"\n        # encoder_outputs (raw keys/values): (seq_len, batch, enc_hid_dim * 2)\n        # decoder_hidden_state (query): (batch, dec_hid_dim)\n\n        seq_len = encoder_outputs.shape[0]\n        batch_size = encoder_outputs.shape[1]\n        encoder_out_dim = encoder_outputs.shape[2]\n        decoder_hidden_dim = decoder_hidden_state.shape[1]\n\n        # --- 1. Project Keys for Scoring ---\n        encoder_outputs_flat = encoder_outputs.view(seq_len * batch_size, encoder_out_dim)\n        projected_keys_flat = self.attn_combine(encoder_outputs_flat)\n        projected_keys = projected_keys_flat.view(seq_len, batch_size, decoder_hidden_dim)\n        projected_keys = projected_keys.permute(1, 0, 2) # (batch, seq_len, dec_hid_dim)\n\n        # --- 2. Calculate Scaled Attention Scores ---\n        query_unsqueezed = decoder_hidden_state.unsqueeze(1) # (batch, 1, dec_hid_dim)\n        # Scores: (batch, 1, D) @ (batch, D, seq_len) -> (batch, 1, seq_len)\n        attn_scores_raw = torch.bmm(query_unsqueezed, projected_keys.transpose(1, 2))\n\n        # --- SCALING ---\n        scale_factor = math.sqrt(decoder_hidden_dim)\n        attn_scores = attn_scores_raw.squeeze(1) / scale_factor # (batch, seq_len)\n        # ---------------\n\n        # --- 3. Calculate Attention Weights ---\n        attn_weights = F.softmax(attn_scores, dim=1) # (batch, seq_len)\n\n        # --- 4. Calculate Raw Context Vector (Weighted Sum of ORIGINAL Encoder Outputs) ---\n        values = encoder_outputs.permute(1, 0, 2) # (batch, seq_len, enc_hid_dim * 2)\n        attn_weights_unsqueezed = attn_weights.unsqueeze(1) # (batch, 1, seq_len)\n        # Context: (batch, 1, seq_len) @ (batch, seq_len, enc_hid_dim * 2) -> (batch, 1, enc_hid_dim * 2)\n        context_vector_raw = torch.bmm(attn_weights_unsqueezed, values)\n        context_vector_raw = context_vector_raw.squeeze(1) # (batch, enc_hid_dim * 2)\n\n        # --- RETURN RAW CONTEXT ---\n        return context_vector_raw # Shape: (batch, enc_hid_dim * 2)\n\n    # --- forward method remains the same as the last version ---\n    # It correctly initializes decoder hidden state and calls compute_attention/decoder\n    def forward(self, src, trg):\n        # ... (Identical to the previous forward method) ...\n        batch_size, trg_len = trg.shape\n        vocab_size = self.decoder.out.out_features\n        outputs = torch.zeros(batch_size, trg_len, vocab_size, device=self.device)\n\n        src = src.T.to(self.device)\n        trg = trg.T.to(self.device)\n        hidden_init = self.encoder.init_hidden(batch_size)\n        encoder_outputs, hidden = self.encoder(src, hidden_init)\n        h_n, c_n = hidden\n        h_n_last = h_n.view(self.encoder.num_layers, 2, batch_size, self.encoder.hidden_size)[-1]\n        c_n_last = c_n.view(self.encoder.num_layers, 2, batch_size, self.encoder.hidden_size)[-1]\n        h_n_combined = torch.cat((h_n_last[0], h_n_last[1]), dim=1)\n        c_n_combined = torch.cat((c_n_last[0], c_n_last[1]), dim=1)\n        decoder_h_init_last = torch.tanh(self.fc_hidden(h_n_combined))\n        decoder_c_init_last = torch.tanh(self.fc_cell(c_n_combined))\n        decoder_h_init = decoder_h_init_last.unsqueeze(0).repeat(self.decoder.num_layers, 1, 1)\n        decoder_c_init = decoder_c_init_last.unsqueeze(0).repeat(self.decoder.num_layers, 1, 1)\n        decoder_hidden = (decoder_h_init.contiguous(), decoder_c_init.contiguous())\n\n        input_token = torch.full((batch_size,), self.sos_idx, dtype=torch.long, device=self.device)\n\n        for t in range(trg_len):\n            decoder_h_last_layer = decoder_hidden[0][-1]\n            # compute_attention now returns raw context (batch, enc_hid_dim * 2)\n            context = self.compute_attention(decoder_h_last_layer, encoder_outputs)\n            # Decoder's forward needs to handle this raw context now\n            output, decoder_hidden = self.decoder(input_token, decoder_hidden, context)\n            outputs[:, t, :] = output\n            input_token = trg[t]\n\n        return outputs","metadata":{"id":"nPvAmc_euarf","trusted":true,"execution":{"iopub.status.busy":"2025-04-30T23:09:00.946307Z","iopub.execute_input":"2025-04-30T23:09:00.946571Z","iopub.status.idle":"2025-04-30T23:09:00.960358Z","shell.execute_reply.started":"2025-04-30T23:09:00.946553Z","shell.execute_reply":"2025-04-30T23:09:00.959590Z"}},"outputs":[],"execution_count":13},{"id":"3Fm5tMpGvBP1","cell_type":"markdown","source":"## ğŸ“ What You Need to Complete\n\n- Implement the `compute_attention()` method to compute a context vector from encoder outputs given the current (decoder) hidden state.\n- Update your `DecoderRNN` class so that it accepts and uses a context vector at every step:\n    - One option is to **concatenate** the context vector with the embedded input token before passing it to the RNN.\n    - Alternatively, you can feed the context into a projection layer together with the decoder hidden state.\n\n---\n\n### ğŸ“Œ Tip: Updated Decoder Signature\n\nYou may need to rewrite your decoder to look like:\n\n```python\ndef forward(self, input_token, hidden, context_vector):\n    ...\n```\n---\n> ğŸ’¡ **Note on Attention Implementation**\n\nYou are given flexibility in how you choose to implement the attention mechanism.\n\nYou may:\n- Implement the **basic dot-product attention** we discussed in class.\n- Or, explore a more expressive variant by adding a **linear projection** to the decoder hidden state or the encoder outputs â€” borrowing an idea from self-attention in Transformers.\n\nAs long as your implementation uses the decoder hidden state to compute attention over the encoder outputs and forms a context vector, you're free to experiment and design the solution that makes most sense to you.\n","metadata":{"id":"3Fm5tMpGvBP1"}},{"id":"lsGSMPTxwj4a","cell_type":"markdown","source":"## 2.2 - Evaluate and Compare\n\nIn this stage, you should use your adapted `Seq2SeqWithAttention` model, train it and evaluate its performance (on the dev set).  ","metadata":{"id":"lsGSMPTxwj4a"}},{"id":"a268f491-9a67-40be-ad7f-281858c1e675","cell_type":"code","source":"# Helper function needed for inference with attention model (REVISED)\ndef infer_sentences_attention(shuffled_sentences, model, word2idx, idx2word, device, max_len=50):\n    \"\"\"\n    Generate predicted sentences using a Seq2Seq model WITH ATTENTION.\n    (Revised to handle Bidirectional Encoder hidden state adaptation)\n    \"\"\"\n    assert isinstance(model, Seq2SeqWithAttention), \"This function requires a Seq2SeqWithAttention model\"\n\n    model.eval()\n    predictions = []\n\n    sos_idx = word2idx['<SOS>']\n    eos_idx = word2idx['<EOS>']\n    pad_idx = word2idx['<PAD>']\n    unk_idx = word2idx['<UNK>']\n\n    with torch.no_grad():\n        for sentence in shuffled_sentences:\n            # --- 1. Prepare Input (Same) ---\n            tokens = tokenize(sentence)\n            token_ids = [word2idx.get(w, unk_idx) for w in tokens]\n            current_len = len(token_ids)\n            if current_len >= max_len:\n                 token_ids = token_ids[:max_len]\n            else:\n                 token_ids += [pad_idx] * (max_len - current_len)\n            src_tensor = torch.tensor([token_ids], dtype=torch.long, device=device) # (1, max_len)\n\n            # --- 2. Encode (Same) ---\n            src_tensor_enc = src_tensor.T.to(device) # (max_len, 1)\n            # Initialize BiEncoder hidden state\n            encoder_hidden_init = model.encoder.init_hidden(batch_size=1)\n            # encoder_hidden = tuple(h.to(device) for h in encoder_hidden_init) # Already on device\n\n            encoder_outputs, hidden = model.encoder(src_tensor_enc, encoder_hidden_init)\n            # encoder_outputs shape: (max_len, 1, encoder_hidden_dim * 2)\n            # hidden tuple (h_n, c_n) shapes: (encoder_num_layers * 2, 1, encoder_hidden_dim) -> (4, 1, 256)\n\n            # --- 3. Adapt Encoder Hidden State for Decoder Initialization (NEW) ---\n            h_n, c_n = hidden # Each shape (4, 1, 256)\n            batch_size = h_n.shape[1] # Should be 1 here\n\n            # Reshape to (num_layers, 2, batch, enc_hid_dim) and take last layer [-1]\n            h_n_last = h_n.view(model.encoder.num_layers, 2, batch_size, model.encoder.hidden_size)[-1]\n            c_n_last = c_n.view(model.encoder.num_layers, 2, batch_size, model.encoder.hidden_size)[-1]\n            # Concatenate fwd/bwd states: (batch, enc_hid_dim * 2)\n            h_n_combined = torch.cat((h_n_last[0], h_n_last[1]), dim=1)\n            c_n_combined = torch.cat((c_n_last[0], c_n_last[1]), dim=1)\n            # Transform using the model's FC layers (defined in Seq2SeqWithAttention.__init__)\n            decoder_h_init_last = torch.tanh(model.fc_hidden(h_n_combined))\n            decoder_c_init_last = torch.tanh(model.fc_cell(c_n_combined))\n            # Repeat/unsqueeze for decoder layers: (decoder_num_layers, batch, dec_hid_dim)\n            decoder_h_init = decoder_h_init_last.unsqueeze(0).repeat(model.decoder.num_layers, 1, 1)\n            decoder_c_init = decoder_c_init_last.unsqueeze(0).repeat(model.decoder.num_layers, 1, 1)\n            # This is the correctly shaped initial hidden state for the decoder\n            decoder_hidden = (decoder_h_init.contiguous(), decoder_c_init.contiguous())\n            # --- End Hidden State Adaptation ---\n\n\n            # --- 4. Decode Loop with Attention (Uses adapted decoder_hidden) ---\n            decoder_input = torch.tensor([sos_idx], device=device) # Start with SOS, shape (1,)\n            # decoder_hidden now has shape (2, 1, 256)\n\n            predicted_token_ids = []\n\n            for t in range(max_len):\n                # --- Attention Calculation ---\n                decoder_h_last_layer = decoder_hidden[0][-1] # Shape: (1, 256)\n                context = model.compute_attention(decoder_h_last_layer, encoder_outputs) # context shape: (1, 256)\n\n                # --- Decoder Step ---\n                # Pass input, the *current* decoder_hidden, and context\n                output, decoder_hidden = model.decoder(decoder_input, decoder_hidden, context)\n                # output shape: (1, vocab_size), decoder_hidden updated shape: (2, 1, 256)\n\n                predicted_token_id = output.argmax(1).item()\n\n                if predicted_token_id == pad_idx: continue\n                predicted_token_ids.append(predicted_token_id)\n                if predicted_token_id == eos_idx: break\n\n                decoder_input = torch.tensor([predicted_token_id], device=device)\n\n            # --- 5. Convert Output (Same) ---\n            predicted_words = [idx2word.get(idx, '<UNK>') for idx in predicted_token_ids]\n            if predicted_words and predicted_words[-1] == '<EOS>':\n                predicted_words = predicted_words[:-1]\n            predicted_sentence = \" \".join(predicted_words)\n            predictions.append(predicted_sentence)\n\n    return predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T23:09:24.553481Z","iopub.execute_input":"2025-04-30T23:09:24.553797Z","iopub.status.idle":"2025-04-30T23:09:24.566773Z","shell.execute_reply.started":"2025-04-30T23:09:24.553778Z","shell.execute_reply":"2025-04-30T23:09:24.565841Z"}},"outputs":[],"execution_count":14},{"id":"30ae6e21-3d35-49e4-87af-da10639eab7c","cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport heapq # For efficiently managing beam candidates\n\ndef infer_sentences_attention_beam(shuffled_sentences, model, word2idx, idx2word, device, max_len=50, beam_width=5):\n    \"\"\"\n    Generate predicted sentences using Beam Search Decoding with an Attention model.\n    \"\"\"\n    assert isinstance(model, Seq2SeqWithAttention), \"This function requires a Seq2SeqWithAttention model\"\n\n    model.eval()\n    predictions = []\n\n    sos_idx = word2idx['<SOS>']\n    eos_idx = word2idx['<EOS>']\n    pad_idx = word2idx['<PAD>']\n    unk_idx = word2idx['<UNK>']\n\n    with torch.no_grad():\n        for sentence in shuffled_sentences:\n            # --- 1. Prepare Input & Encode (Same as before) ---\n            tokens = tokenize(sentence)\n            token_ids = [word2idx.get(w, unk_idx) for w in tokens]\n            current_len = len(token_ids)\n            if current_len >= max_len:\n                 token_ids = token_ids[:max_len]\n            else:\n                 token_ids += [pad_idx] * (max_len - current_len)\n            src_tensor = torch.tensor([token_ids], dtype=torch.long, device=device)\n            src_tensor_enc = src_tensor.T.to(device)\n            encoder_hidden_init = model.encoder.init_hidden(batch_size=1)\n            encoder_outputs, hidden = model.encoder(src_tensor_enc, encoder_hidden_init)\n\n            # Adapt encoder hidden state for decoder initialization (same as before)\n            h_n, c_n = hidden\n            batch_size = h_n.shape[1] # Should be 1\n            h_n_last = h_n.view(model.encoder.num_layers, 2, batch_size, model.encoder.hidden_size)[-1]\n            c_n_last = c_n.view(model.encoder.num_layers, 2, batch_size, model.encoder.hidden_size)[-1]\n            h_n_combined = torch.cat((h_n_last[0], h_n_last[1]), dim=1)\n            c_n_combined = torch.cat((c_n_last[0], c_n_last[1]), dim=1)\n            decoder_h_init_last = torch.tanh(model.fc_hidden(h_n_combined))\n            decoder_c_init_last = torch.tanh(model.fc_cell(c_n_combined))\n            decoder_h_init = decoder_h_init_last.unsqueeze(0).repeat(model.decoder.num_layers, 1, 1)\n            decoder_c_init = decoder_c_init_last.unsqueeze(0).repeat(model.decoder.num_layers, 1, 1)\n            decoder_hidden = (decoder_h_init.contiguous(), decoder_c_init.contiguous())\n\n            # --- 2. Beam Search Decoding ---\n            # Start with SOS token. Store beams as (log_prob, sequence_indices, hidden_state_tuple)\n            # Use negative log prob because heapq is a min-heap\n            initial_beam = (-0.0, [sos_idx], decoder_hidden)\n            beams = [initial_beam]\n            completed_sequences = []\n\n            for _ in range(max_len):\n                new_beams = []\n                for log_prob, seq_indices, current_hidden in beams:\n                    # If last token was EOS, this sequence is complete\n                    if seq_indices[-1] == eos_idx:\n                        completed_sequences.append((log_prob, seq_indices))\n                        # Prune this beam if we have enough completed ones? Optional.\n                        continue # Don't expand completed sequences\n\n                    # Prepare decoder input (last token of the current sequence)\n                    decoder_input = torch.tensor([seq_indices[-1]], device=device)\n\n                    # --- Attention Calculation ---\n                    decoder_h_last_layer = current_hidden[0][-1]\n                    context = model.compute_attention(decoder_h_last_layer, encoder_outputs)\n\n                    # --- Decoder Step ---\n                    output_logits, next_hidden = model.decoder(decoder_input, current_hidden, context)\n\n                    # Get top k next tokens (use log_softmax for probabilities)\n                    log_probs = F.log_softmax(output_logits, dim=1) # Shape (1, vocab_size)\n                    top_log_probs, top_indices = torch.topk(log_probs, beam_width, dim=1) # Shapes (1, k), (1, k)\n\n                    # Expand the current beam with the top k candidates\n                    for i in range(beam_width):\n                        next_token_idx = top_indices[0, i].item()\n                        next_log_prob = top_log_probs[0, i].item()\n\n                        new_log_prob = log_prob + next_log_prob\n                        new_seq_indices = seq_indices + [next_token_idx]\n\n                        # Add to potential new beams heap. Store as (neg_log_prob, ...) for min-heap.\n                        heapq.heappush(new_beams, (-new_log_prob, new_seq_indices, next_hidden))\n\n\n                # Prune new_beams to keep only the top beam_width overall best\n                # Take top beam_width elements using nsmallest or by popping\n                beams = heapq.nsmallest(beam_width, new_beams)\n                # Convert back to (pos_log_prob, ...) - negate the first element\n                beams = [(-neg_log_prob, seq, hid) for neg_log_prob, seq, hid in beams]\n\n\n                # Check if all active beams ended in EOS (or if beams list is empty)\n                # Optimization: If top beam is much better than others, can stop early.\n                if not beams: # Should not happen if beam_width > 0\n                    break\n                all_ended = all(b[1][-1] == eos_idx for b in beams)\n                if all_ended:\n                    completed_sequences.extend(beams) # Add all current beams as completed\n                    break\n\n\n            # If no sequences completed properly (e.g., max_len reached), add current beams\n            if not completed_sequences:\n                 completed_sequences.extend(beams)\n\n            # Sort completed sequences by log probability (higher is better)\n            completed_sequences.sort(key=lambda x: x[0], reverse=True)\n\n            # --- 3. Convert Best Output ---\n            # Get the top sequence (log_prob, sequence_indices, hidden_state_tuple)\n            # We only need the sequence_indices for the final output.\n            best_seq_indices = completed_sequences[0][1] # Get the sequence indices from the best beam\n\n            predicted_token_ids = best_seq_indices[1:] # Remove the initial SOS token\n\n            predicted_words = [idx2word.get(idx, '<UNK>') for idx in predicted_token_ids]\n            if predicted_words and predicted_words[-1] == '<EOS>':\n                predicted_words = predicted_words[:-1]\n            predicted_sentence = \" \".join(predicted_words)\n            predictions.append(predicted_sentence)\n\n    return predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T23:09:38.436515Z","iopub.execute_input":"2025-04-30T23:09:38.436808Z","iopub.status.idle":"2025-04-30T23:09:38.452262Z","shell.execute_reply.started":"2025-04-30T23:09:38.436787Z","shell.execute_reply":"2025-04-30T23:09:38.451235Z"}},"outputs":[],"execution_count":15},{"id":"RRjxs544y1NK","cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport math\nimport numpy as np # Needed for inf\nimport os # Needed for file path\n\n# --- Training Function with Early Stopping, Gradient Clipping ---\n# (Define this function if you haven't already, or modify your existing one)\ndef train_model_early_stopping(model, train_loader, dev_loader, optimizer, criterion, device, clip, patience, model_path, num_epochs=30):\n    \"\"\"Trains the model with early stopping based on dev loss.\"\"\"\n    model.to(device)\n    train_losses = []\n    dev_losses = []\n\n    best_dev_loss = float('inf')\n    patience_counter = 0\n\n    print(f\"Starting training for up to {num_epochs} epochs...\")\n    print(f\"Early stopping patience: {patience} epochs. Saving best model to: {model_path}\")\n\n    for epoch in range(num_epochs):\n        model.train()\n        epoch_train_loss = 0\n\n        for src, trg in train_loader:\n            src, trg = src.to(device), trg.to(device)\n\n            optimizer.zero_grad()\n            output = model(src, trg) # Forward pass\n\n            output_flat = output.view(-1, output.shape[-1])\n            trg_flat = trg.view(-1)\n\n            loss = criterion(output_flat, trg_flat)\n            loss.backward() # Calculate gradients\n            torch.nn.utils.clip_grad_norm_(model.parameters(), clip) # Gradient Clipping\n            optimizer.step() # Update weights\n            epoch_train_loss += loss.item()\n\n        avg_train_loss = epoch_train_loss / len(train_loader)\n        train_losses.append(avg_train_loss)\n\n        # Evaluate on dev set\n        model.eval()\n        epoch_dev_loss = 0\n        with torch.no_grad():\n            for src, trg in dev_loader:\n                src, trg = src.to(device), trg.to(device)\n                output = model(src, trg)\n                output_flat = output.view(-1, output.shape[-1])\n                trg_flat = trg.view(-1)\n                loss = criterion(output_flat, trg_flat)\n                epoch_dev_loss += loss.item()\n\n        avg_dev_loss = epoch_dev_loss / len(dev_loader)\n        dev_losses.append(avg_dev_loss)\n\n        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Dev Loss: {avg_dev_loss:.4f}\", end='')\n\n        # Early Stopping Check\n        if avg_dev_loss < best_dev_loss:\n            best_dev_loss = avg_dev_loss\n            patience_counter = 0\n            torch.save(model.state_dict(), model_path)\n            print(\" | Dev Loss Improved, Saving Model\")\n        else:\n            patience_counter += 1\n            print(f\" | Patience Counter: {patience_counter}/{patience}\")\n            if patience_counter >= patience:\n                print(\"Early stopping triggered.\")\n                break # Stop training\n\n    print(f\"Training finished. Best Dev Loss: {best_dev_loss:.4f}\")\n    # Ensure the function returns losses up to the point of stopping\n    return train_losses, dev_losses\n\n# --- Instantiate and Train Block ---\n\nprint(\"\\n--- Training Seq2Seq Model with Bidirectional Encoder + Attention (Early Stopping) ---\")\n\n# 1. Define Hyperparameters\nembedding_size = 128\nhidden_size = 256   # Per direction for encoder, and decoder hidden size\nencoder_layers = 2\ndecoder_layers = 2\nmax_epochs = 30      # Maximum epochs to run if no early stopping\nlearning_rate = 0.001\nclip_value = 1.0     # Gradient clipping\ndropout_prob = 0.4   # Increased dropout\nweight_decay_val = 1e-5 # Added weight decay\nearly_stopping_patience = 5 # Stop after 5 epochs of no dev loss improvement\nbest_model_save_path = 'best_bi_attn_model.pt' # Path to save the best model\n\n# Ensure these are defined from previous cells:\nvocab_size = len(word2idx)\npad_idx = word2idx[PAD]\nsos_idx = word2idx[SOS]\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntrain_loader, dev_loader\n# dev_df, word2idx, idx2word, max_len\n\n# 2. Instantiate Models\nencoder_bi_attn = EncoderRNN(vocab_size, embedding_size, hidden_size, num_layers=encoder_layers)\n# Pass encoder_hidden_dim and dropout_p\ndecoder_bi_attn = DecoderRNN(vocab_size, embedding_size, hidden_size,\n                             encoder_hidden_dim=hidden_size,\n                             num_layers=decoder_layers,\n                             dropout_p=dropout_prob) # Pass dropout here\nmodel_bi_attn = Seq2SeqWithAttention(encoder_bi_attn, decoder_bi_attn, sos_idx, device)\nmodel_bi_attn.to(device)\n\nprint(f\"Bidirectional Attention model created on device: {device}\")\nprint(f\"Model Parameters: {sum(p.numel() for p in model_bi_attn.parameters() if p.requires_grad):,}\")\n\n\n# 3. Define Optimizer (with weight decay) and Criterion\noptimizer_bi_attn = optim.Adam(model_bi_attn.parameters(), lr=learning_rate, weight_decay=weight_decay_val) # Added weight_decay\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n\n\n# 4. Train the Bidirectional Attention Model with Early Stopping\ntrain_losses_bi_attn, dev_losses_bi_attn = train_model_early_stopping(\n    model_bi_attn, train_loader, dev_loader,\n    optimizer_bi_attn, criterion, device, clip_value,\n    patience=early_stopping_patience,\n    model_path=best_model_save_path,\n    num_epochs=max_epochs # Pass max epochs\n)\n\n# 5. Plot Losses\nprint(\"\\nPlotting bidirectional attention model losses...\")\n# Make sure plot_losses function exists and can handle potentially fewer epochs due to early stopping\nplot_losses(train_losses_bi_attn, dev_losses_bi_attn)\n\n\n# --- Evaluation Block (Using BEST saved model) ---\n\nprint(\"\\n--- Evaluating BEST Bidirectional Attention Model on Dev Set ---\")\n\n# --- BLOCK 1: Load the previously determined best model (Epoch 8) ---\nprint(\"\\n--- Loading Best Model for Fine-tuning ---\")\n# Instantiate a new model instance\nencoder_fine_tune = EncoderRNN(vocab_size, embedding_size, hidden_size, num_layers=encoder_layers)\ndecoder_fine_tune = DecoderRNN(vocab_size, embedding_size, hidden_size,\n                               encoder_hidden_dim=hidden_size,\n                               num_layers=decoder_layers,\n                               dropout_p=dropout_prob)\nmodel_fine_tune = Seq2SeqWithAttention(encoder_fine_tune, decoder_fine_tune, sos_idx, device)\n\nbest_model_save_path = 'best_bi_attn_model.pt' # Path where Epoch 8 model was saved\nif os.path.exists(best_model_save_path):\n    print(f\"Loading best model weights from: {best_model_save_path}\")\n    model_fine_tune.load_state_dict(torch.load(best_model_save_path))\n    model_fine_tune.to(device)\nelse:\n    print(f\"Error: Best model file not found at {best_model_save_path}. Cannot fine-tune.\")\n\n# --- BLOCK 2: Fine-tune with lower learning rate ---\nprint(\"\\n--- Fine-tuning with Lower Learning Rate ---\")\n\n# Define new optimizer with lower LR and same weight decay\nfine_tune_lr = 0.0001 # Significantly lower LR\noptimizer_fine_tune = optim.Adam(model_fine_tune.parameters(), lr=fine_tune_lr, weight_decay=weight_decay_val)\n\n# Define new save path for the fine-tuned best model\nfine_tuned_best_model_save_path = 'fine_tuned_best_bi_attn_model.pt'\nfine_tune_max_epochs = 15 # Train for up to 15 more epochs\nfine_tune_patience = 5 # Reset patience for fine-tuning\n\n# Ensure train_model_early_stopping function is defined\ntrain_losses_fine_tune, dev_losses_fine_tune = train_model_early_stopping(\n    model_fine_tune, train_loader, dev_loader,\n    optimizer_fine_tune, criterion, device, clip_value, # Use same criterion/clip\n    patience=fine_tune_patience,\n    model_path=fine_tuned_best_model_save_path, # Save to new path\n    num_epochs=fine_tune_max_epochs\n)\n\n# --- BLOCK 3: Evaluate the FINE-TUNED Best Model ---\nprint(\"\\n--- Evaluating FINE-TUNED Best Bidirectional Attention Model ---\")\n\n# Instantiate final model instance\nencoder_final = EncoderRNN(vocab_size, embedding_size, hidden_size, num_layers=encoder_layers)\ndecoder_final = DecoderRNN(vocab_size, embedding_size, hidden_size,\n                           encoder_hidden_dim=hidden_size,\n                           num_layers=decoder_layers,\n                           dropout_p=dropout_prob)\nmodel_final_best = Seq2SeqWithAttention(encoder_final, decoder_final, sos_idx, device)\n\n# Load weights saved during fine-tuning\nif os.path.exists(fine_tuned_best_model_save_path):\n    print(f\"Loading fine-tuned best model weights from: {fine_tuned_best_model_save_path}\")\n    model_final_best.load_state_dict(torch.load(fine_tuned_best_model_save_path))\n    model_final_best.to(device)\nelse:\n    print(f\"Error: Fine-tuned best model file not found. Evaluating last state.\")\n    model_final_best = model_fine_tune # Evaluate the last state from fine-tuning\n\n# Get dev sentences\n# dev_shuffled_sentences = dev_df['input_sentence'].tolist()\n# dev_target_sentences = dev_df['target_sentence'].tolist()\n\n# Run inference using BEAM SEARCH\nprint(\"Running inference on development set (FINE-TUNED Best Bi-Attention Model with BEAM SEARCH)...\")\nmodel_final_best.eval()\nbeam_k = 10 # Use beam width 10 again\ndev_predictions_final = infer_sentences_attention_beam(\n    dev_shuffled_sentences,\n    model_final_best,\n    word2idx,\n    idx2word,\n    device,\n    max_len,\n    beam_width=beam_k\n)\nprint(f\"Inference complete (Beam Width = {beam_k}).\")\n\n# Evaluate the predictions\nprint(\"\\nEvaluating dev set predictions (FINE-TUNED Best Bi-Attention Model with BEAM SEARCH):\")\ndev_accuracy_final = evaluate_sentence_predictions(dev_predictions_final, dev_target_sentences)\n\n# Print samples\nprint(\"\\nSample Dev Predictions vs Targets (FINE-TUNED Best Bi-Attention Model with BEAM SEARCH):\")\nfor i in range(min(5, len(dev_predictions_final))):\n    print(f\"Input:    {dev_shuffled_sentences[i]}\")\n    print(f\"Predicted: {dev_predictions_final[i]}\")\n    print(f\"Target:   {dev_target_sentences[i]}\")\n    print(\"-\" * 20)\n\nprint(\"--- FINE-TUNED Evaluation Finished ---\")","metadata":{"id":"RRjxs544y1NK","trusted":true,"execution":{"iopub.status.busy":"2025-04-30T23:09:50.647490Z","iopub.execute_input":"2025-04-30T23:09:50.648389Z","iopub.status.idle":"2025-05-01T00:40:24.960805Z","shell.execute_reply.started":"2025-04-30T23:09:50.648348Z","shell.execute_reply":"2025-05-01T00:40:24.959704Z"}},"outputs":[{"name":"stdout","text":"\n--- Training Seq2Seq Model with Bidirectional Encoder + Attention (Early Stopping) ---\nBidirectional Attention model created on device: cuda\nModel Parameters: 12,427,347\nStarting training for up to 30 epochs...\nEarly stopping patience: 5 epochs. Saving best model to: best_bi_attn_model.pt\nEpoch 1/30 | Train Loss: 4.2793 | Dev Loss: 2.9274 | Dev Loss Improved, Saving Model\nEpoch 2/30 | Train Loss: 2.4780 | Dev Loss: 2.1106 | Dev Loss Improved, Saving Model\nEpoch 3/30 | Train Loss: 1.5738 | Dev Loss: 1.7463 | Dev Loss Improved, Saving Model\nEpoch 4/30 | Train Loss: 1.0239 | Dev Loss: 1.5912 | Dev Loss Improved, Saving Model\nEpoch 5/30 | Train Loss: 0.7084 | Dev Loss: 1.5128 | Dev Loss Improved, Saving Model\nEpoch 6/30 | Train Loss: 0.5527 | Dev Loss: 1.4907 | Dev Loss Improved, Saving Model\nEpoch 7/30 | Train Loss: 0.4744 | Dev Loss: 1.4810 | Dev Loss Improved, Saving Model\nEpoch 8/30 | Train Loss: 0.4123 | Dev Loss: 1.4569 | Dev Loss Improved, Saving Model\nEpoch 9/30 | Train Loss: 0.3686 | Dev Loss: 1.4264 | Dev Loss Improved, Saving Model\nEpoch 10/30 | Train Loss: 0.3318 | Dev Loss: 1.4383 | Patience Counter: 1/5\nEpoch 11/30 | Train Loss: 0.3023 | Dev Loss: 1.4267 | Patience Counter: 2/5\nEpoch 12/30 | Train Loss: 0.2785 | Dev Loss: 1.4334 | Patience Counter: 3/5\nEpoch 13/30 | Train Loss: 0.2592 | Dev Loss: 1.4196 | Dev Loss Improved, Saving Model\nEpoch 14/30 | Train Loss: 0.2438 | Dev Loss: 1.4076 | Dev Loss Improved, Saving Model\nEpoch 15/30 | Train Loss: 0.2302 | Dev Loss: 1.3984 | Dev Loss Improved, Saving Model\nEpoch 16/30 | Train Loss: 0.2166 | Dev Loss: 1.3966 | Dev Loss Improved, Saving Model\nEpoch 17/30 | Train Loss: 0.2057 | Dev Loss: 1.3930 | Dev Loss Improved, Saving Model\nEpoch 18/30 | Train Loss: 0.1984 | Dev Loss: 1.4195 | Patience Counter: 1/5\nEpoch 19/30 | Train Loss: 0.1931 | Dev Loss: 1.3739 | Dev Loss Improved, Saving Model\nEpoch 20/30 | Train Loss: 0.1855 | Dev Loss: 1.3803 | Patience Counter: 1/5\nEpoch 21/30 | Train Loss: 0.1797 | Dev Loss: 1.3822 | Patience Counter: 2/5\nEpoch 22/30 | Train Loss: 0.1706 | Dev Loss: 1.3500 | Dev Loss Improved, Saving Model\nEpoch 23/30 | Train Loss: 0.1684 | Dev Loss: 1.3729 | Patience Counter: 1/5\nEpoch 24/30 | Train Loss: 0.1666 | Dev Loss: 1.3640 | Patience Counter: 2/5\nEpoch 25/30 | Train Loss: 0.1604 | Dev Loss: 1.3648 | Patience Counter: 3/5\nEpoch 26/30 | Train Loss: 0.1594 | Dev Loss: 1.3609 | Patience Counter: 4/5\nEpoch 27/30 | Train Loss: 0.1552 | Dev Loss: 1.3316 | Dev Loss Improved, Saving Model\nEpoch 28/30 | Train Loss: 0.1518 | Dev Loss: 1.3481 | Patience Counter: 1/5\nEpoch 29/30 | Train Loss: 0.1509 | Dev Loss: 1.3287 | Dev Loss Improved, Saving Model\nEpoch 30/30 | Train Loss: 0.1473 | Dev Loss: 1.3262 | Dev Loss Improved, Saving Model\nTraining finished. Best Dev Loss: 1.3262\n\nPlotting bidirectional attention model losses...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"\n--- Evaluating BEST Bidirectional Attention Model on Dev Set ---\n\n--- Loading Best Model for Fine-tuning ---\nLoading best model weights from: best_bi_attn_model.pt\n\n--- Fine-tuning with Lower Learning Rate ---\nStarting training for up to 15 epochs...\nEarly stopping patience: 5 epochs. Saving best model to: fine_tuned_best_bi_attn_model.pt\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/3421859032.py:155: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model_fine_tune.load_state_dict(torch.load(best_model_save_path))\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/15 | Train Loss: 0.1952 | Dev Loss: 1.2861 | Dev Loss Improved, Saving Model\nEpoch 2/15 | Train Loss: 0.1149 | Dev Loss: 1.2773 | Dev Loss Improved, Saving Model\nEpoch 3/15 | Train Loss: 0.0750 | Dev Loss: 1.2714 | Dev Loss Improved, Saving Model\nEpoch 4/15 | Train Loss: 0.0524 | Dev Loss: 1.2790 | Patience Counter: 1/5\nEpoch 5/15 | Train Loss: 0.0402 | Dev Loss: 1.2825 | Patience Counter: 2/5\nEpoch 6/15 | Train Loss: 0.0342 | Dev Loss: 1.2815 | Patience Counter: 3/5\nEpoch 7/15 | Train Loss: 0.0313 | Dev Loss: 1.2788 | Patience Counter: 4/5\nEpoch 8/15 | Train Loss: 0.0299 | Dev Loss: 1.2890 | Patience Counter: 5/5\nEarly stopping triggered.\nTraining finished. Best Dev Loss: 1.2714\n\n--- Evaluating FINE-TUNED Best Bidirectional Attention Model ---\nLoading fine-tuned best model weights from: fine_tuned_best_bi_attn_model.pt\nRunning inference on development set (FINE-TUNED Best Bi-Attention Model with BEAM SEARCH)...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/3421859032.py:195: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model_final_best.load_state_dict(torch.load(fine_tuned_best_model_save_path))\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/3421859032.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0mbeam_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;31m# Use beam width 10 again\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m dev_predictions_final = infer_sentences_attention_beam(\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0mdev_shuffled_sentences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m     \u001b[0mmodel_final_best\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0mword2idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'dev_shuffled_sentences' is not defined"],"ename":"NameError","evalue":"name 'dev_shuffled_sentences' is not defined","output_type":"error"}],"execution_count":16},{"id":"3994ed65-59fa-41b6-9b44-3d13495371b6","cell_type":"code","source":"dev_shuffled_sentences = dev_df['input_sentence'].tolist()\ndev_target_sentences = dev_df['target_sentence'].tolist()\nprint(\"Running inference on development set (FINE-TUNED Best Bi-Attention Model with BEAM SEARCH)...\")\nmodel_final_best.eval()\nbeam_k = 10 # Use beam width 10 again\ndev_predictions_final = infer_sentences_attention_beam(\n    dev_shuffled_sentences,\n    model_final_best,\n    word2idx,\n    idx2word,\n    device,\n    max_len,\n    beam_width=beam_k\n)\nprint(f\"Inference complete (Beam Width = {beam_k}).\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T00:42:19.080660Z","iopub.execute_input":"2025-05-01T00:42:19.080999Z","iopub.status.idle":"2025-05-01T00:54:16.910844Z","shell.execute_reply.started":"2025-05-01T00:42:19.080976Z","shell.execute_reply":"2025-05-01T00:54:16.909867Z"}},"outputs":[{"name":"stdout","text":"Running inference on development set (FINE-TUNED Best Bi-Attention Model with BEAM SEARCH)...\nInference complete (Beam Width = 10).\n\nEvaluating dev set predictions (FINE-TUNED Best Bi-Attention Model with BEAM SEARCH):\nExact match accuracy: 0.00%\n\nSample Dev Predictions vs Targets (FINE-TUNED Best Bi-Attention Model with BEAM SEARCH):\nInput:    upset? What got him so\nPredicted: what <UNK> him got up.\nTarget:   What got him so upset?\n--------------------\nInput:    planting neighbor grass his is yard. My in new\nPredicted: my <UNK> <UNK> is sleeping in his son.\nTarget:   My neighbor is planting new grass in his yard.\n--------------------\nInput:    I It morning saw Mr. yesterday was Carter. that\nPredicted: yesterday i saw it was a dangerous <UNK>\nTarget:   It was yesterday morning that I saw Mr. Carter.\n--------------------\nInput:    as clearly as can. you speak Please\nPredicted: \nTarget:   Please speak as clearly as you can.\n--------------------\nInput:    orange? have Can I this\nPredicted: i can have <UNK> <UNK>\nTarget:   Can I have this orange?\n--------------------\n--- FINE-TUNED Evaluation Finished ---\n","output_type":"stream"}],"execution_count":17},{"id":"7965d0a0-e248-452e-910d-0e502d2a2b73","cell_type":"code","source":"# Evaluate the predictions\nprint(\"\\nEvaluating dev set predictions (FINE-TUNED Best Bi-Attention Model with BEAM SEARCH):\")\ndev_accuracy_final = evaluate_sentence_predictions(dev_predictions_final, dev_target_sentences)\n\n# Print samples\nprint(\"\\nSample Dev Predictions vs Targets (FINE-TUNED Best Bi-Attention Model with BEAM SEARCH):\")\nfor i in range(min(5, len(dev_predictions_final))):\n    print(f\"Input:    {dev_shuffled_sentences[i]}\")\n    print(f\"Predicted: {dev_predictions_final[i]}\")\n    print(f\"Target:   {dev_target_sentences[i]}\")\n    print(\"-\" * 20)\n\nprint(\"--- FINE-TUNED Evaluation Finished ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T00:58:01.749888Z","iopub.execute_input":"2025-05-01T00:58:01.750228Z","iopub.status.idle":"2025-05-01T00:58:01.757657Z","shell.execute_reply.started":"2025-05-01T00:58:01.750208Z","shell.execute_reply":"2025-05-01T00:58:01.756759Z"}},"outputs":[{"name":"stdout","text":"\nEvaluating dev set predictions (FINE-TUNED Best Bi-Attention Model with BEAM SEARCH):\nExact match accuracy: 6.37%\n\nSample Dev Predictions vs Targets (FINE-TUNED Best Bi-Attention Model with BEAM SEARCH):\nInput:    upset? What got him so\nPredicted: what <UNK> him got up.\nTarget:   What got him so upset?\n--------------------\nInput:    planting neighbor grass his is yard. My in new\nPredicted: my <UNK> <UNK> is sleeping in his son.\nTarget:   My neighbor is planting new grass in his yard.\n--------------------\nInput:    I It morning saw Mr. yesterday was Carter. that\nPredicted: yesterday i saw it was a dangerous <UNK>\nTarget:   It was yesterday morning that I saw Mr. Carter.\n--------------------\nInput:    as clearly as can. you speak Please\nPredicted: \nTarget:   Please speak as clearly as you can.\n--------------------\nInput:    orange? have Can I this\nPredicted: i can have <UNK> <UNK>\nTarget:   Can I have this orange?\n--------------------\n--- FINE-TUNED Evaluation Finished ---\n","output_type":"stream"}],"execution_count":19},{"id":"88e3a5ab-7765-4292-a81d-5db05c6254f5","cell_type":"code","source":"import os\n\n# --- Step 1: Confirm Best Model File Location ---\n\n# Define the path where the best fine-tuned model was saved\nfine_tuned_best_model_save_path = 'fine_tuned_best_bi_attn_model.pt'\nkaggle_output_dir = '/kaggle/working/' # Standard Kaggle output directory\nmodel_full_path = os.path.join(kaggle_output_dir, fine_tuned_best_model_save_path)\n\n\nprint(f\"\\n--- Confirming Best Model Location ---\")\nif os.path.exists(model_full_path):\n    # File is saved within the Kaggle environment\n    print(f\"Best model file is saved at: {model_full_path}\")\n    print(\"After committing the notebook, find this file in the 'Output' section.\")\nelse:\n    # Check the current directory just in case\n    if os.path.exists(fine_tuned_best_model_save_path):\n         model_full_path = os.path.abspath(fine_tuned_best_model_save_path)\n         print(f\"Best model file is saved at: {model_full_path}\")\n         print(\"After committing the notebook, find this file in the 'Output' section.\")\n    else:\n        print(f\"Error: Best model file '{fine_tuned_best_model_save_path}' not found in {kaggle_output_dir} or current dir.\")\n\n\n# --- Step 2: Generate Predictions for the Test Set ---\n\nprint(f\"\\n--- Generating Test Set Predictions using Best Model ---\")\n\n# Make sure the necessary variables are defined from previous cells:\n# model_final_best (loaded with best weights), word2idx, idx2word, device, max_len\n# And the inference function: infer_sentences_attention_beam\n\n# Specify the beam width used for final predictions\nfinal_beam_k = 10\n\n# --- Load the actual test set ---\n# Ensure test_no_target.csv is uploaded as a Kaggle Dataset or added to the notebook input\n# Adjust path if necessary (e.g., /kaggle/input/your-dataset-name/test_no_target.csv)\ntest_set_path = '/kaggle/input/test-no-target/test_no_target.csv' # Check if this path is correct in Kaggle!\nif os.path.exists(test_set_path):\n    print(f\"Loading test data from: {test_set_path}\")\n    test_df = pd.read_csv(test_set_path)\n    test_shuffled_sentences = test_df['input_sentence'].tolist()\n\n    # Ensure the final best model is loaded and on the correct device\n    # (Should already be loaded in model_final_best from the evaluation block)\n    model_final_best.to(device)\n    model_final_best.eval() # Set to evaluation mode\n\n    # Generate predictions using beam search\n    print(f\"Running inference on test set (Beam Width = {final_beam_k})...\")\n    test_predictions = infer_sentences_attention_beam(\n        test_shuffled_sentences,\n        model_final_best,\n        word2idx,\n        idx2word,\n        device,\n        max_len,\n        beam_width=final_beam_k\n    )\n    print(\"Test set inference complete.\")\n\n    # --- Step 3: Save Predictions to CSV in Kaggle Output Directory ---\n    # Define the full path within the /kaggle/working/ directory\n    predictions_csv_filename = 'seq2seq_with_attention_predictions.csv'\n    predictions_csv_path = os.path.join(kaggle_output_dir, predictions_csv_filename)\n\n    print(f\"Saving test predictions to: {predictions_csv_path}\")\n\n    # Create DataFrame\n    predictions_df = pd.DataFrame({'predicted_sentence': test_predictions})\n\n    # Save to CSV\n    predictions_df.to_csv(predictions_csv_path, index=False)\n    print(\"Predictions saved successfully.\")\n    print(f\"Find '{predictions_csv_filename}' in the 'Output' section after committing.\")\n\nelse:\n    print(f\"Error: Test set file '{test_set_path}' not found.\")\n    print(\"Please add 'test_no_target.csv' as input data to your Kaggle notebook.\")\n\nprint(\"\\n--- Process Complete ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T01:02:05.039194Z","iopub.execute_input":"2025-05-01T01:02:05.039466Z","iopub.status.idle":"2025-05-01T01:19:00.887118Z","shell.execute_reply.started":"2025-05-01T01:02:05.039447Z","shell.execute_reply":"2025-05-01T01:19:00.886370Z"}},"outputs":[{"name":"stdout","text":"\n--- Confirming Best Model Location ---\nBest model file is saved at: /kaggle/working/fine_tuned_best_bi_attn_model.pt\nAfter committing the notebook, find this file in the 'Output' section.\n\n--- Generating Test Set Predictions using Best Model ---\nLoading test data from: /kaggle/input/test-no-target/test_no_target.csv\nRunning inference on test set (Beam Width = 10)...\nTest set inference complete.\nSaving test predictions to: /kaggle/working/seq2seq_with_attention_predictions.csv\nPredictions saved successfully.\nFind 'seq2seq_with_attention_predictions.csv' in the 'Output' section after committing.\n\n--- Process Complete ---\n","output_type":"stream"}],"execution_count":22},{"id":"b54e29c4","cell_type":"markdown","source":"## ğŸ“Œ Submission Instructions:\nSubmit your completed notebook along with two files of predictions over the test set - `seq2seq_predictions.csv` and `seq2seq_with_attention_predictions.csv`.\n\nI will run the evaluation over the held-out test set.\n\nYou should email all of these files with an email specifying the names and ids of the team (couples).\n\nâœ… **Good luck!**\n","metadata":{"id":"b54e29c4"}}]}